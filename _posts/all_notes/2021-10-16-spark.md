---
layout: post
title:  "Apache Spark"
date:   2021-10-16 09:29:20 +0700
categories: post
---

[Good tutorial](https://data-flair.training/blogs/spark-tutorials-home/#tutorials)

The need for Spark came from the limitations of the MapReduce.  Those limitations are:

1. Unsuitable in real-time processing --> MapReduce is based on batch processing
2. Unsuitable for trivial operations such as Filter and Joins due to the key-value patern
3. Unfit for large data on network --> takes a lot of time to copy the data which may cause network bandwidth issues, but it works well locally on the node where the data resides.
4. Unsuitable with Online Transaction Processing (OLTP) because it is batch based and hence has latency.
5. Unfit for processing graphs
6. Unfit for iterative executions --> e.g. K-means needs to rerun itself on the data in order to get better resullts, whereas MapReduce runs from the start every time (stateless executor). 

There was no powerful engine in the industry, that can process the data both in real-time and batch mode. Also, there was a requirement that one engine can respond in sub-second and perform in-memory processing.
Therefore, Apache Spark programming enters, it is a powerful open source engine. Since, it offers **real-time stream processing, interactive processing, graph processing, in-memory processing as well as batch processing**. 
Even with very fast speed, ease of use and standard interface. Basically, these features create the difference between Hadoop and Spark. 

Spark gives the following additional features:

1. Suitable for real-time processing
2. Suitable for trivial operations
3. Can fir large data on network
4. Suitable for OLTP
5. Suitable for graphs
6. Suitable for iterative executions

Spark is about 100 times faster than MapReduce.

Spark Components:

1. Spark Core and Resilient Distributed Databases (RDDs)
2. Spark SQL
3. Spark Streaming 
4. Machine Learning Library (MLlib)
5. GraphX

